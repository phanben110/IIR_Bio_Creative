{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "file_path = '/Users/benphan/NCKU/IIR-Lab/BioCreative/Data/Demo_Bug.pubTator'\n",
    "\n",
    "# Step 1: Open the file\n",
    "rawDatas = [] \n",
    "preprocess_datas = []\n",
    "text =  \"\"\n",
    "with open(file_path, 'r') as file:\n",
    "    preprocess = []\n",
    "    begin = True\n",
    "    # Step 2: Read and process the contents\n",
    "    for line in file:\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields) == 5:\n",
    "           rawDatas.append(text)\n",
    "           preprocess_datas.append(preprocess)\n",
    "          #  print(preprocess)\n",
    "           text = \"\"\n",
    "           preprocess = []\n",
    "        elif '|t|' in fields[0]: \n",
    "          begin = False\n",
    "          position = fields[0].index('|t|')\n",
    "          text += str(fields[0][position + 3:])\n",
    "        elif '|a|' in fields[0]: \n",
    "          position = fields[0].index('|a|')\n",
    "          text = str(text + \" \" + fields[0][position + 3:]) \n",
    "        elif fields[0] == \"\": \n",
    "    \n",
    "          continue\n",
    "        else:\n",
    "          if fields[1].isdigit() and fields[2].isdigit():\n",
    "            if len(fields) == 6: \n",
    "              # if \"|\" in fields[5]:\n",
    "              #   identifier_list = fields[5].split('|')\n",
    "              if \",\" in fields[5]:\n",
    "                identifier_list = fields[5].split(',')\n",
    "              else:  \n",
    "                identifier_list = [fields[5]]\n",
    "              for identifier in identifier_list:\n",
    "                preprocess.append([fields[0], fields[1], fields[2], fields[3], fields[4], identifier])\n",
    "              # preprocess.append(fields) \n",
    "\n",
    "datas = []\n",
    "for i, rawData in enumerate(rawDatas):\n",
    "  sentences = nltk.sent_tokenize(rawData)\n",
    "  position = []\n",
    "  for j, sentence in enumerate(sentences):\n",
    "    index = rawData.index(sentence)\n",
    "    position.append([j , index, index + len(sentence)])\n",
    "  data_dict = {\n",
    "                \"raw_data\": rawData, \n",
    "                \"sentences\": sentences, \n",
    "                \"position\": position,\n",
    "                \"preprocess\": preprocess_datas[i].copy(),\n",
    "                \"clean_data\": preprocess_datas[i] \n",
    "                }  \n",
    "  datas.append(data_dict) \n",
    "\n",
    "\n",
    "for i, data in enumerate(datas): \n",
    "  # print(\"Done\")\n",
    "  for k, item in enumerate (data[\"preprocess\"]):\n",
    "    for j, position in enumerate(data[\"position\"]):\n",
    "      if int(item[1]) >= int(position[1]) and int(item[2]) <= int(position[2]):\n",
    "        if len(item) == 6: \n",
    "          item.append(data[\"sentences\"][position[0]]) \n",
    "        else: \n",
    "          print(k)\n",
    "          print(len(data[\"preprocess\"]))\n",
    "        #datas[i][\"sentences\"][position[0]]\n",
    "\n",
    "\n",
    "with open('ben_deptrai.pubtator', 'w') as file:\n",
    "    for data in datas:\n",
    "        for clean_data in data[\"preprocess\"]:\n",
    "            if (len(clean_data) < 7):\n",
    "               print(clean_data)\n",
    "            line = '\\t'.join(clean_data) + '\\n'\n",
    "            file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given identifiers as strings\n",
    "identifier1 = \"D015458|D016399|32323232\"\n",
    "identifier2 = \"D015458,D016399,32323232\"\n",
    "identifier3 = \"D015458\"\n",
    "\n",
    "# Split the identifiers using either '|' or ',' as separators\n",
    "identifier_list1 = identifier1.split('|')  # Split using '|'\n",
    "identifier_list2 = identifier2.split(',')   # Split using ','\n",
    "identifier_list3 = [identifier3]            # For single identifier, create a list with one element\n",
    "\n",
    "# Merge all the lists into a single list\n",
    "all_identifiers = identifier_list1 + identifier_list2 + identifier_list3\n",
    "\n",
    "# Resulting list\n",
    "print(all_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier1 = \"D015458|D016399|32323232\" \n",
    "\"|\" in identifier1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "\n",
    "list_file = os.listdir('/Users/benphan/NCKU/IIR-Lab/BioCreative/NewData') \n",
    "for file_path in list_file:\n",
    "  output_path = os.path.join('/Users/benphan/NCKU/IIR-Lab/BioCreative/NewData_Add_Sentence', f\"{file_path[:-9]}_add_sentence.pubtator\") \n",
    "  file_path = os.path.join('/Users/benphan/NCKU/IIR-Lab/BioCreative/NewData', file_path)  \n",
    "\n",
    "\n",
    "  # Step 1: Open the file\n",
    "  rawDatas = [] \n",
    "  preprocess_datas = []\n",
    "  text =  \"\"\n",
    "  with open(file_path, 'r') as file:\n",
    "      preprocess = []\n",
    "      # Step 2: Read and process the contents\n",
    "      for line in file:\n",
    "          fields = line.strip().split('\\t')\n",
    "          if fields[0] == \"\": \n",
    "            rawDatas.append(text)\n",
    "            preprocess_datas.append(preprocess)\n",
    "            #  print(preprocess)\n",
    "            text = \"\"\n",
    "            preprocess = []\n",
    "          elif '|t|' in fields[0]: \n",
    "            position = fields[0].index('|t|')\n",
    "            text += str(fields[0][position + 3:])\n",
    "          elif '|a|' in fields[0]: \n",
    "            position = fields[0].index('|a|')\n",
    "            text = str(text + \" \" + fields[0][position + 3:]) \n",
    "          else:\n",
    "            if fields[1].isdigit() and fields[2].isdigit():\n",
    "              if len(fields) == 6: \n",
    "                # if \"|\" in fields[5]:\n",
    "                #   identifier_list = fields[5].split('|')\n",
    "                if \",\" in fields[5]:\n",
    "                  identifier_list = fields[5].split(',')\n",
    "                else:  \n",
    "                  identifier_list = [fields[5]]\n",
    "                for identifier in identifier_list:\n",
    "                  preprocess.append([fields[0], fields[1], fields[2], fields[3], fields[4], identifier])\n",
    "                # preprocess.append(fields) \n",
    "\n",
    "  datas = []\n",
    "  for i, rawData in enumerate(rawDatas):\n",
    "    sentences = nltk.sent_tokenize(rawData)\n",
    "    position = []\n",
    "    for j, sentence in enumerate(sentences):\n",
    "      index = rawData.index(sentence)\n",
    "      position.append([j , index, index + len(sentence)])\n",
    "    data_dict = {\n",
    "                  \"raw_data\": rawData, \n",
    "                  \"sentences\": sentences, \n",
    "                  \"position\": position,\n",
    "                  \"preprocess\": preprocess_datas[i].copy(),\n",
    "                  \"clean_data\": preprocess_datas[i] \n",
    "                  }  \n",
    "    datas.append(data_dict) \n",
    "\n",
    "  for i, data in enumerate(datas): \n",
    "    lenght = len(data[\"preprocess\"]) \n",
    "    for item in data[\"preprocess\"]:\n",
    "\n",
    "      for j, position in enumerate(data[\"position\"]):\n",
    "        \n",
    "        if (int(item[1]) >= int(position[1]) ) and (int(item[2]) <= int(position[2]) ):\n",
    "          # print(len(item)) \n",
    "          item.append(data[\"sentences\"][position[0]]) \n",
    "\n",
    "          #datas[i][\"sentences\"][position[0]]\n",
    "\n",
    "  with open(output_path, 'w') as file:\n",
    "      for data in datas:\n",
    "          for clean_data in data[\"preprocess\"]:\n",
    "            if (len(clean_data) < 7):\n",
    "              print(clean_data)\n",
    "            line = '\\t'.join(clean_data) + '\\n'\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
